<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>scrapy源码2：scheduler的源码分析 | 张素英的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="scheduler核心Scheduler主要负责scrapy请求队列的管理，即进队与出队。进一步来说，会涉及到队列的选择，队列去重，序列化。">
<meta property="og:type" content="article">
<meta property="og:title" content="scrapy源码2：scheduler的源码分析">
<meta property="og:url" content="http://example.com/2020/07/01/spider/scrapy/20200701_scheduler/index.html">
<meta property="og:site_name" content="张素英的博客">
<meta property="og:description" content="scheduler核心Scheduler主要负责scrapy请求队列的管理，即进队与出队。进一步来说，会涉及到队列的选择，队列去重，序列化。">
<meta property="og:locale">
<meta property="article:published_time" content="2020-07-01T02:35:00.000Z">
<meta property="article:modified_time" content="2020-10-23T08:53:48.000Z">
<meta property="article:author" content="Mr.zhang">
<meta property="article:tag" content="python">
<meta property="article:tag" content="爬虫">
<meta property="article:tag" content="scrapy">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="张素英的博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">张素英的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-spider/scrapy/20200701_scheduler" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/07/01/spider/scrapy/20200701_scheduler/" class="article-date">
  <time class="dt-published" datetime="2020-07-01T02:35:00.000Z" itemprop="datePublished">2020-07-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/python/">python</a>►<a class="article-category-link" href="/categories/python/%E7%88%AC%E8%99%AB/">爬虫</a>►<a class="article-category-link" href="/categories/python/%E7%88%AC%E8%99%AB/scrapy/">scrapy</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      scrapy源码2：scheduler的源码分析
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="scheduler核心"><a href="#scheduler核心" class="headerlink" title="scheduler核心"></a>scheduler核心</h3><p>Scheduler主要负责scrapy请求队列的管理，即进队与出队。进一步来说，会涉及到队列的选择，队列去重，序列化。</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from_crawler(cls, crawler):</span><br><span class="line">        settings = crawler.settings</span><br><span class="line">        dupefilter_cls = load_object(settings[<span class="string">&#x27;DUPEFILTER_CLASS&#x27;</span>])</span><br><span class="line">        dupefilter = dupefilter_cls.from_settings(settings)</span><br><span class="line">        pqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_PRIORITY_QUEUE&#x27;</span>])</span><br><span class="line">        dqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_DISK_QUEUE&#x27;</span>])</span><br><span class="line">        mqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_MEMORY_QUEUE&#x27;</span>])</span><br><span class="line">        logunser = settings.getbool(<span class="string">&#x27;LOG_UNSERIALIZABLE_REQUESTS&#x27;</span>, settings.getbool(<span class="string">&#x27;SCHEDULER_DEBUG&#x27;</span>))</span><br><span class="line">        <span class="keyword">return</span> cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,</span><br><span class="line">                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass, mqclass=mqclass)</span><br></pre></td></tr></table></figure>

<p>创建了4个对象，分别是dupefilter,pqclass,dqclass,mqclass。</p>
<h4 id="1-dupefilter过滤器（url去重）"><a href="#1-dupefilter过滤器（url去重）" class="headerlink" title="1. dupefilter过滤器（url去重）"></a>1. dupefilter过滤器（url去重）</h4><p>DUPEFILTER_CLASS = ‘scrapy.dupefilters.RFPDupeFilter’这个类的含义是”Request Fingerprint duplicates filter”，请求指纹副本过滤。也就是对每个request请求做一个指纹，保证相同的请求有相同的指纹。对重复的请求进行过滤。包含查询字符串、cookies字段的相同url也会被去重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RFPDupeFilter</span>(<span class="params">BaseDupeFilter</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span>(<span class="params">self, request</span>):</span></span><br><span class="line">        fp = self.request_fingerprint(request)</span><br><span class="line">        <span class="keyword">if</span> fp <span class="keyword">in</span> self.fingerprints:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        self.fingerprints.add(fp)</span><br><span class="line">        <span class="keyword">if</span> self.file:</span><br><span class="line">            self.file.write(fp + os.linesep)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_fingerprint</span>(<span class="params">self, request</span>):</span></span><br><span class="line">        <span class="keyword">return</span> request_fingerprint(request)</span><br></pre></td></tr></table></figure>
<p>scrapy默认的去重方案：利用request生成fingerprint, 存入set，每次利用set判断，如果用了 disk queue 追加至文件</p>
<h4 id="2-pqclass优先级队列"><a href="#2-pqclass优先级队列" class="headerlink" title="2. pqclass优先级队列"></a>2. pqclass优先级队列</h4><p>SCHEDULER_PRIORITY_QUEUE = ‘queuelib.PriorityQueue’这是一个优先级队列，使用的是开源的第三方queuelib.它的作用就是对request请求按优先级进行排序，这样我们可以对不同重要性的URL指定优先级（通过设置Request的priority属性）。优先级是一个整数，虽然queuelib使用小的数做为高优化级，但是由于scheduler入队列时取了负值，所以对于我们来说，数值越大优先级越高。</p>
<h4 id="3-dqclass支持序列化的后进先出的磁盘队列"><a href="#3-dqclass支持序列化的后进先出的磁盘队列" class="headerlink" title="3. dqclass支持序列化的后进先出的磁盘队列"></a>3. dqclass支持序列化的后进先出的磁盘队列</h4><p>SCHEDULER_DISK_QUEUE = ‘scrapy.squeues.PickleLifoDiskQueue’<br>这是一个支持序列化的后进先出的磁盘队列。主要用来帮助我们在停止爬虫后可以接着上一次继续开始爬虫。</p>
<h4 id="4-mqclass后进先出的内存队列"><a href="#4-mqclass后进先出的内存队列" class="headerlink" title="4. mqclass后进先出的内存队列"></a>4. mqclass后进先出的内存队列</h4><p>SCHEDULER_MEMORY_QUEUE = ‘scrapy.squeues.LifoMemoryQueue’从名字上看，是后进先出的内存队列。这个队列是为了使用2中的队列而存在的，不必单独分析。</p>
<h3 id="scheduler源码解释笔记"><a href="#scheduler源码解释笔记" class="headerlink" title="scheduler源码解释笔记"></a>scheduler源码解释笔记</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scheduler.py</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join, exists</span><br><span class="line"></span><br><span class="line"><span class="comment"># request_to_dict: 将请求对象转换成dict，如果给定了一个spider，它将尝试找出回调中使用的spider方法的名称，并将其存储为回调。</span></span><br><span class="line"><span class="comment"># request_from_dict：从dict创建请求对象，如果给定了一个spider，它将尝试解析在spider中查找同名方法的回调。</span></span><br><span class="line"><span class="keyword">from</span> scrapy.utils.reqser <span class="keyword">import</span> request_to_dict, request_from_dict</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.misc <span class="keyword">import</span> load_object, create_instance</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.job <span class="keyword">import</span> job_dir</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(__name__)  <span class="comment"># 获得一个全局的logger对象。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Scheduler</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dupefilter, jobdir=<span class="literal">None</span>, dqclass=<span class="literal">None</span>, mqclass=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 logunser=<span class="literal">False</span>, stats=<span class="literal">None</span>, pqclass=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.df = dupefilter  <span class="comment"># 去重模块    默认利用set在内存去重</span></span><br><span class="line">        self.dqdir = self._dqdir(jobdir)  <span class="comment"># 磁盘队列路径  持久化队列至硬盘</span></span><br><span class="line">        self.pqclass = pqclass  <span class="comment"># 带优先级队列    默认来自queuelib</span></span><br><span class="line">        self.dqclass = dqclass  <span class="comment"># 磁盘队列  持久化队列至硬盘</span></span><br><span class="line">        self.mqclass = mqclass  <span class="comment"># 内存队列  默认来自queuelib</span></span><br><span class="line">        self.logunser = logunser</span><br><span class="line">        self.stats = stats  <span class="comment"># 状态记录  状态记录通用模块</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从crawler的设置获取各个属性， 然后使用load_object 获取对应类。</span></span><br><span class="line">    <span class="comment"># 主要有以下几个名词， 调度优先级队列，调度磁盘队列，调度内存队列。调度debug开启是否，日志非序列化请求，重复类。</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span>  <span class="comment"># 实例化入口    scrapy风格的实例化入口</span></span><br><span class="line">        settings = crawler.settings</span><br><span class="line">        dupefilter_cls = load_object(settings[<span class="string">&#x27;DUPEFILTER_CLASS&#x27;</span>])</span><br><span class="line">        dupefilter = create_instance(dupefilter_cls, settings, crawler)</span><br><span class="line">        pqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_PRIORITY_QUEUE&#x27;</span>])  <span class="comment"># &#x27;queuelib.PriorityQueue&#x27;</span></span><br><span class="line">        dqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_DISK_QUEUE&#x27;</span>])  <span class="comment"># &#x27;scrapy.squeues.PickleLifoDiskQueue&#x27;</span></span><br><span class="line">        mqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_MEMORY_QUEUE&#x27;</span>])  <span class="comment"># &#x27;scrapy.squeues.LifoMemoryQueue&#x27;</span></span><br><span class="line">        logunser = settings.getbool(<span class="string">&#x27;LOG_UNSERIALIZABLE_REQUESTS&#x27;</span>, settings.getbool(<span class="string">&#x27;SCHEDULER_DEBUG&#x27;</span>))</span><br><span class="line">        <span class="keyword">return</span> cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,</span><br><span class="line">                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass, mqclass=mqclass)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取是否还有请求没处理。 返回true或者false.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">has_pending_requests</span>(<span class="params">self</span>):</span>  <span class="comment"># 检查队列数    指向len</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self) &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打开调度器方法： 设置当前的爬虫，设置当前的内存队列，磁盘队列，内存队列初始值为调度优先级队列。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span>(<span class="params">self, spider</span>):</span>  <span class="comment"># 初始化队列    scrapy模块的初始化入口</span></span><br><span class="line">        self.spider = spider</span><br><span class="line">        self.mqs = self.pqclass(self._newmq)</span><br><span class="line">        self.dqs = self._dq() <span class="keyword">if</span> self.dqdir <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> self.df.<span class="built_in">open</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭调度器方法： 判断dqs， 关闭dqs，打开active.json文件， 把prios信息写进去。关闭df</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self, reason</span>):</span>  <span class="comment">#     安全退出接口  scrapy模块的安全入口</span></span><br><span class="line">        <span class="keyword">if</span> self.dqs:</span><br><span class="line">            prios = self.dqs.close()</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(join(self.dqdir, <span class="string">&#x27;active.json&#x27;</span>), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                json.dump(prios, f)</span><br><span class="line">        <span class="keyword">return</span> self.df.close(reason)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># enqueue_request: 请求进队列</span></span><br><span class="line">    <span class="comment"># 如果请求是不过滤的，过滤器df的请求处理过。记录日志， 返回false</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">enqueue_request</span>(<span class="params">self, request</span>):</span>  <span class="comment"># 进队api    调度进队</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> request.dont_filter <span class="keyword">and</span> self.df.request_seen(request):</span><br><span class="line">            self.df.log(request, self.spider)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        dqok = self._dqpush(request)  <span class="comment"># self._dqpush 这个是磁盘队列加入这个请求</span></span><br><span class="line">        <span class="keyword">if</span> dqok:  <span class="comment"># 如果成功，就给统计信息的disk的对应爬虫加1</span></span><br><span class="line">            self.stats.inc_value(<span class="string">&#x27;scheduler/enqueued/disk&#x27;</span>, spider=self.spider)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 其他情况的话，就给统计信息的memory的对应爬虫加1</span></span><br><span class="line">            self._mqpush(request)</span><br><span class="line">            self.stats.inc_value(<span class="string">&#x27;scheduler/enqueued/memory&#x27;</span>, spider=self.spider)</span><br><span class="line">        self.stats.inc_value(<span class="string">&#x27;scheduler/enqueued&#x27;</span>, spider=self.spider)  <span class="comment"># 总的也需要加1，然后返回true</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># next_request：从队列里取出数据进行处理</span></span><br><span class="line">    <span class="comment"># 获取下一个请求， 先从内存队列mqs里面pop一个，给memory加1，如果内存中为空就从磁盘队列dq里面pop一个。</span></span><br><span class="line">    <span class="comment"># 然后disk加1，如果request不为空， 就给dequed加1</span></span><br><span class="line">    <span class="comment"># 注意这个方法和上个方法， 一个是入，一个是出 的。</span></span><br><span class="line">    <span class="comment"># 统计信息也是， 一个统计到en队列中， 一个统计到de队列去。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_request</span>(<span class="params">self</span>):</span>  <span class="comment"># 出队api    调度出队，t优先从内存队列里取，然后才是磁盘队列</span></span><br><span class="line">        request = self.mqs.pop()</span><br><span class="line">        <span class="keyword">if</span> request:</span><br><span class="line">            self.stats.inc_value(<span class="string">&#x27;scheduler/dequeued/memory&#x27;</span>, spider=self.spider)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            request = self._dqpop()</span><br><span class="line">            <span class="keyword">if</span> request:</span><br><span class="line">                self.stats.inc_value(<span class="string">&#x27;scheduler/dequeued/disk&#x27;</span>, spider=self.spider)</span><br><span class="line">        <span class="keyword">if</span> request:</span><br><span class="line">            self.stats.inc_value(<span class="string">&#x27;scheduler/dequeued&#x27;</span>, spider=self.spider)</span><br><span class="line">        <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 三目运算， 如果磁盘队列不为空的话， 就是磁盘队列加内存队列的长度， 否则就是内存队列的长度。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dqs) + <span class="built_in">len</span>(self.mqs) <span class="keyword">if</span> self.dqs <span class="keyword">else</span> <span class="built_in">len</span>(self.mqs)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    下面的几个方法，_dqpush，_mqpush，_dqpop，_newmq，_newdq，_dq，_dqdir</span></span><br><span class="line"><span class="string">    从方法名字，看看有push,you pop， new ， 大概可以知道这个是构造，添加，删除操作。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    _newmq: 构造一个内存队列， _newdq: 构造一个磁盘队列。 </span></span><br><span class="line"><span class="string">    具体的列可以看出从setting里面读取过来的。实例化的在这个方面里面的。 返回值就是对应的对象。 </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    _mqpush: 内存队列里面push一个请求， 优先级为请求的负值</span></span><br><span class="line"><span class="string">             </span></span><br><span class="line"><span class="string">    _dqpush: 核心就是请求转化为字典， 然后把dict放到磁盘队列中， 如果有异常，说明是无法序列化请求，构造msg信息。 </span></span><br><span class="line"><span class="string">             记录警告信息。并记录序列化失败个数，最总返回true，有异常那就返回none</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    _dqpop: 从磁盘队列获取pop一个dict，然后将dict转为request。 返回回去。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    _dqdir: 获取dqdir，如果有设置的话， 就会创建一个目录，并返回这个目录</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dqpush</span>(<span class="params">self, request</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.dqs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            reqd = request_to_dict(request, self.spider)</span><br><span class="line">            self.dqs.push(reqd, -request.priority)</span><br><span class="line">        <span class="keyword">except</span> ValueError <span class="keyword">as</span> e:  <span class="comment"># non serializable request</span></span><br><span class="line">            <span class="keyword">if</span> self.logunser:</span><br><span class="line">                msg = (<span class="string">&quot;Unable to serialize request: %(request)s - reason:&quot;</span></span><br><span class="line">                       <span class="string">&quot; %(reason)s - no more unserializable requests will be&quot;</span></span><br><span class="line">                       <span class="string">&quot; logged (stats being collected)&quot;</span>)</span><br><span class="line">                logger.warning(msg, &#123;<span class="string">&#x27;request&#x27;</span>: request, <span class="string">&#x27;reason&#x27;</span>: e&#125;,</span><br><span class="line">                               exc_info=<span class="literal">True</span>, extra=&#123;<span class="string">&#x27;spider&#x27;</span>: self.spider&#125;)</span><br><span class="line">                self.logunser = <span class="literal">False</span></span><br><span class="line">            self.stats.inc_value(<span class="string">&#x27;scheduler/unserializable&#x27;</span>,</span><br><span class="line">                                 spider=self.spider)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_mqpush</span>(<span class="params">self, request</span>):</span></span><br><span class="line">        self.mqs.push(request, -request.priority)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dqpop</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.dqs:</span><br><span class="line">            d = self.dqs.pop()</span><br><span class="line">            <span class="keyword">if</span> d:</span><br><span class="line">                <span class="keyword">return</span> request_from_dict(d, self.spider)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_newmq</span>(<span class="params">self, priority</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.mqclass()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_newdq</span>(<span class="params">self, priority</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.dqclass(join(self.dqdir, <span class="string">&#x27;p%s&#x27;</span> % priority))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dq</span>(<span class="params">self</span>):</span></span><br><span class="line">        activef = join(self.dqdir, <span class="string">&#x27;active.json&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> exists(activef):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(activef) <span class="keyword">as</span> f:</span><br><span class="line">                prios = json.load(f)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prios = ()</span><br><span class="line">        q = self.pqclass(self._newdq, startprios=prios)</span><br><span class="line">        <span class="keyword">if</span> q:</span><br><span class="line">            logger.info(<span class="string">&quot;Resuming crawl (%(queuesize)d requests scheduled)&quot;</span>,</span><br><span class="line">                        &#123;<span class="string">&#x27;queuesize&#x27;</span>: <span class="built_in">len</span>(q)&#125;, extra=&#123;<span class="string">&#x27;spider&#x27;</span>: self.spider&#125;)</span><br><span class="line">        <span class="keyword">return</span> q</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dqdir</span>(<span class="params">self, jobdir</span>):</span></span><br><span class="line">        <span class="keyword">if</span> jobdir:</span><br><span class="line">            dqdir = join(jobdir, <span class="string">&#x27;requests.queue&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> exists(dqdir):</span><br><span class="line">                os.makedirs(dqdir)</span><br><span class="line">            <span class="keyword">return</span> dqdir</span><br><span class="line"></span><br></pre></td></tr></table></figure>







      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/07/01/spider/scrapy/20200701_scheduler/" data-id="ckqd8hrsy002sustg95v2ajph" data-title="scrapy源码2：scheduler的源码分析" class="article-share-link">Teilen</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scrapy/" rel="tag">scrapy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/07/03/spider/scrapy/20200703_%20scraper/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          scrapy源码3：scraper的源码分析
        
      </div>
    </a>
  
  
    <a href="/2020/06/30/spider/scrapy/20200630_engine/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">scrapy源码1：engine的源码分析</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Kategorien</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/python/fastapi/">fastapi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/flask/">flask</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/python%E5%9F%BA%E7%A1%80/">python基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/python%E6%A1%88%E4%BE%8B/">python案例</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/%E7%88%AC%E8%99%AB/">爬虫</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/python/%E7%88%AC%E8%99%AB/pyppeteer/">pyppeteer</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/%E7%88%AC%E8%99%AB/scrapy/">scrapy</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/%E7%88%AC%E8%99%AB/selenium/">selenium</a></li></ul></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/fastapi/" rel="tag">fastapi</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flask/" rel="tag">flask</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyppeteer/" rel="tag">pyppeteer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python3/" rel="tag">python3</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python%E5%9F%BA%E7%A1%80/" rel="tag">python基础</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python%E6%A1%88%E4%BE%8B/" rel="tag">python案例</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scrapy/" rel="tag">scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/selenium/" rel="tag">selenium</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BB%A3%E7%A0%81%E9%9B%A8/" rel="tag">代码雨</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%8D%E7%88%AC/" rel="tag">反爬</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/fastapi/" style="font-size: 10px;">fastapi</a> <a href="/tags/flask/" style="font-size: 10px;">flask</a> <a href="/tags/pyppeteer/" style="font-size: 12px;">pyppeteer</a> <a href="/tags/python/" style="font-size: 18px;">python</a> <a href="/tags/python3/" style="font-size: 10px;">python3</a> <a href="/tags/python%E5%9F%BA%E7%A1%80/" style="font-size: 20px;">python基础</a> <a href="/tags/python%E6%A1%88%E4%BE%8B/" style="font-size: 12px;">python案例</a> <a href="/tags/scrapy/" style="font-size: 14px;">scrapy</a> <a href="/tags/selenium/" style="font-size: 12px;">selenium</a> <a href="/tags/%E4%BB%A3%E7%A0%81%E9%9B%A8/" style="font-size: 10px;">代码雨</a> <a href="/tags/%E5%8F%8D%E7%88%AC/" style="font-size: 10px;">反爬</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 16px;">爬虫</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/06/26/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2020/12/02/my_application/20201202_%E5%B0%86excel%E4%B8%AD%E7%9A%84%E5%86%85%E5%AE%B9%E5%AD%98%E5%85%A5mysql/">将excel中的内容存入mysql</a>
          </li>
        
          <li>
            <a href="/2020/10/15/my_application/20201015_%E4%BB%A3%E7%A0%81%E9%9B%A8/">用python实现代码雨（电影黑客帝国里的效果，代码可直接运行）</a>
          </li>
        
          <li>
            <a href="/2020/09/29/spider/20200929_%E4%B8%89%E7%A7%8D%E5%8A%A0%E5%AF%86%E6%96%B9%E5%BC%8F/">三种加密方式 sha1加密、MD5加密、Base64加密 (附H5源码和js源码)</a>
          </li>
        
          <li>
            <a href="/2020/07/15/spider/scrapy/20200715_%20ajaxcrawl/">scrapy源码12 - ajaxcrawl</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Mr.zhang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>